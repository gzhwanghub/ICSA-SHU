功能：（1）实现稀疏2D-TDGA通信模型，（2）稀疏通信模型与通用一致性ADMM结合，构造基于稀疏通用一致性ADMM的分布式训练框架，（3）提出稀疏计算和动态惩罚项参数选择，加快对偶上升算法的收敛速度。（4） 提出早停止机制，减少节点间同步等待时间，加快基于ADMM的分布式ML框架的训练速度。
该代码增加了4个基于AllReduce的同步算法，Ring-AllReduce，Hierarchical Ring-AllReduce，Torus-AllReduce, 2D-Torus-AllReduce
20220221提交到天河进行测试
主要测试：
	1. 通信模型对比：提出的稀疏2D-TDGA同步算法与现在流行的Ring-AllReduce,Hierarchical Ring-AllReduce,Torus-AllReduce等非稀疏算法比较。主要创新点：<1> 2D-TDGA通信模型；<2> 稀疏格式传输
	2. 稀疏通信与稀疏计算：通过解决带l1正则的逻辑回归问题，数据集采用URL和webspam。将提出的稀疏2D-TDGA算法与通用一致性ADMM算法结合,构造通用一致性ADMM算法框架，与现有的全局一致性ADMM框架对比训练时间差异。主要创新点：<1> 稀疏2D-TDGA算法与通用一致性ADMM算法结合,构造通用一致性ADMM算法框架； <2> 更新稀疏梯度，减少对偶上升算法计算时间。<3> 自适应惩罚项参数选择，提高算法收敛速度。 
	3. 子问题求解：对TRON算法求解子问题中节点速度差异性，提出早停止机制，在不影响算法收敛性的同时，减少分布式算法同步等待时间，加快基于ADMM的分布式ML框架的训练速度。主要创新点：<1> 子问题早停止
20230501测试一些基于AllReduce的稀疏算法
主要优化点：稀疏传输和减少内存占用
 

